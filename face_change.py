# -*- coding: utf-8 -*-
"""face_change

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xbo13iB5Vh5ixEuBKI2fXVUNCcbPPP72
"""

# ==========================================
# [í†µí•© ì™„ì„±ë³¸] V2 ì˜ìƒë¶„ì„ + ì‚¬ì§„í•©ì„± + ê³ ì •ì£¼ì†Œ
# ==========================================
import os
import uvicorn
from fastapi import FastAPI, File, UploadFile, Form
from fastapi.middleware.cors import CORSMiddleware
from pyngrok import ngrok
import nest_asyncio
import cv2
import numpy as np
import insightface
from insightface.app import FaceAnalysis
import base64
import shutil
import tempfile

# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
os.system("pip install fastapi uvicorn pyngrok nest_asyncio insightface onnxruntime-gpu opencv-python-headless multipart moviepy")

# 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
if not os.path.exists("inswapper_128.onnx"):
    os.system("wget https://huggingface.co/ezioruan/inswapper_128.onnx/resolve/main/inswapper_128.onnx -O inswapper_128.onnx")

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

print("â³ AI ëª¨ë¸ ë¡œë”© ì¤‘...")
face_analyzer = FaceAnalysis(name='buffalo_l')
face_analyzer.prepare(ctx_id=0, det_size=(640, 640))
swapper = insightface.model_zoo.get_model('./inswapper_128.onnx', download=False, download_zip=False)
print("âœ… ì¤€ë¹„ ì™„ë£Œ!")

def img_to_base64(img):
    _, encoded_img = cv2.imencode(".jpg", img)
    return base64.b64encode(encoded_img).decode("utf-8")

def compute_sim(feat1, feat2):
    return np.dot(feat1, feat2) / (np.linalg.norm(feat1) * np.linalg.norm(feat2))

detected_faces_db = {}

@app.get("/")
def read_root():
    return {"status": "Photo & Video Server Ready"}

# ==========================================
# [ì¶”ê°€ëœ ê¸°ëŠ¥] ì‚¬ì§„ í•©ì„± (Image Swap)
# ==========================================
@app.post("/swap_image")
async def swap_image(source: UploadFile = File(...), target: UploadFile = File(...)):
    print("ğŸ“¸ ì‚¬ì§„ í•©ì„± ìš”ì²­ ë„ì°©")

    # ì´ë¯¸ì§€ ì½ê¸°
    source_bytes = await source.read()
    target_bytes = await target.read()
    source_img = cv2.imdecode(np.frombuffer(source_bytes, np.uint8), cv2.IMREAD_COLOR)
    target_img = cv2.imdecode(np.frombuffer(target_bytes, np.uint8), cv2.IMREAD_COLOR)

    # ì–¼êµ´ ë¶„ì„
    source_faces = face_analyzer.get(source_img)
    target_faces = face_analyzer.get(target_img)

    if not source_faces or not target_faces:
        return {"error": "ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    # ê°€ì¥ í° ì–¼êµ´ ê¸°ì¤€
    source_face = sorted(source_faces, key=lambda x: x.bbox[2]*x.bbox[3])[-1]
    target_face = sorted(target_faces, key=lambda x: x.bbox[2]*x.bbox[3])[-1]

    # í•©ì„±
    res = swapper.get(target_img, target_face, source_face, paste_back=True)

    return {"image": f"data:image/jpeg;base64,{img_to_base64(res)}"}


# ==========================================
# [ê¸°ì¡´ ê¸°ëŠ¥] ì˜ìƒ ë¶„ì„ (V2 ë¡œì§ ìœ ì§€)
# ==========================================
@app.post("/analyze")
async def analyze_video(video: UploadFile = File(...)):
    print("ğŸ¬ ì˜ìƒ ë¶„ì„ ì‹œì‘ (V2 í•„í„°ë§ ì ìš©)...")
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as temp_video:
        shutil.copyfileobj(video.file, temp_video)
        video_path = temp_video.name

    cap = cv2.VideoCapture(video_path)
    unique_faces = []

    frame_count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret: break
        frame_count += 1

        # 15í”„ë ˆì„ë§ˆë‹¤ ê²€ì‚¬
        if frame_count % 15 != 0: continue

        faces = face_analyzer.get(frame)
        for face in faces:
            # [í•„í„° 1] ì‘ì€ ì–¼êµ´ ë¬´ì‹œ
            box = face.bbox.astype(int)
            width = box[2] - box[0]
            height = box[3] - box[1]
            if width < 50 or height < 50: continue

            # [í•„í„° 2] ì¤‘ë³µ ì œê±° (ìœ ì‚¬ë„ 0.3 ê¸°ì¤€)
            max_sim = -1
            for unique in unique_faces:
                sim = compute_sim(face.embedding, unique['embedding'])
                if sim > max_sim:
                    max_sim = sim

            if max_sim > 0.3:
                pass # ì´ë¯¸ ìˆëŠ” ì‚¬ëŒ
            else:
                # ìƒˆë¡œìš´ ì‚¬ëŒ ë“±ë¡
                face_img = frame[box[1]:box[3], box[0]:box[2]]
                unique_faces.append({
                    "embedding": face.embedding,
                    "thumb": img_to_base64(face_img)
                })

        if len(unique_faces) >= 5: break

    cap.release()
    detected_faces_db['current_video_path'] = video_path
    detected_faces_db['faces'] = unique_faces

    response_list = []
    for idx, item in enumerate(unique_faces):
        response_list.append({
            "id": idx,
            "image": f"data:image/jpeg;base64,{item['thumb']}"
        })
    print(f"âœ… ë¶„ì„ ì™„ë£Œ: ì´ {len(unique_faces)}ëª… ë°œê²¬")
    return {"faces": response_list}

# ==========================================
# [ê¸°ì¡´ ê¸°ëŠ¥] ì˜ìƒ ë³€í™˜
# ==========================================
@app.post("/swap_video")
async def swap_video_process(target_face: UploadFile = File(...), face_id: int = Form(...)):
    print(f"ğŸ”„ ì˜ìƒ ë³€í™˜ ì‹œì‘...")
    if 'current_video_path' not in detected_faces_db:
        return {"error": "ì˜ìƒì„ ë¨¼ì € ë¶„ì„í•´ì£¼ì„¸ìš”."}

    # íƒ€ê²Ÿ(ë‚´ ì–¼êµ´) ë¶„ì„
    target_bytes = await target_face.read()
    target_img = cv2.imdecode(np.frombuffer(target_bytes, np.uint8), cv2.IMREAD_COLOR)
    target_faces = face_analyzer.get(target_img)
    if not target_faces: return {"error": "ë‚´ ì‚¬ì§„ ì˜¤ë¥˜"}
    source_face = sorted(target_faces, key=lambda x: x.bbox[2]*x.bbox[3])[-1]

    target_embedding = detected_faces_db['faces'][face_id]['embedding']
    video_path = detected_faces_db['current_video_path']

    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    output_path = "output_video.mp4"
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret: break

        faces = face_analyzer.get(frame)
        for face in faces:
            sim = compute_sim(face.embedding, target_embedding)
            if sim > 0.35:
                frame = swapper.get(frame, face, source_face, paste_back=True)
        out.write(frame)

    cap.release()
    out.release()

    with open(output_path, "rb") as f:
        video_b64 = base64.b64encode(f.read()).decode("utf-8")

    return {"video": f"data:video/mp4;base64,{video_b64}"}

# ==========================================
# 4. ì„œë²„ ì‹¤í–‰ (ê³ ì • ë„ë©”ì¸ ìˆ˜ì •ë¨!)
# ==========================================
ngrok.kill()
MY_DOMAIN = "leisha-uncommiserating-motherly.ngrok-free.dev"

# ì¤‘ìš”: domain=MY_DOMAIN ì„ ê¼­ ë„£ì–´ì¤˜ì•¼ í•©ë‹ˆë‹¤!
public_url = ngrok.connect(8000, domain=MY_DOMAIN).public_url

print(f"\nğŸš€ í†µí•© ì„œë²„ ì‹œì‘: {public_url}")

nest_asyncio.apply()
config = uvicorn.Config(app, port=8000)
server = uvicorn.Server(config)
await server.serve()

from google.colab import drive
drive.mount('/content/drive')